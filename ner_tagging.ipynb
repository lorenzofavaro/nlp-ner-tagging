{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3d8dd84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from conllu import parse_incr, TokenList\n",
    "from enum import Enum\n",
    "from typing import Iterator, List, Dict, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9fb17e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmoothingStrategy(Enum):\n",
    "    first = 1\n",
    "\n",
    "\n",
    "smoothing_strategy = None\n",
    "train = open(\"data/train.conllu\", \"r\", encoding=\"utf-8\")\n",
    "test = open(\"data/test.conllu\", \"r\", encoding=\"utf-8\")\n",
    "val = open(\"data/val.conllu\", \"r\", encoding=\"utf-8\")\n",
    "tags = ['START', 'O', 'I-LOC', 'I-MISC', 'I-ORG', 'I-PER', 'B-LOC', 'B-MISC', 'B-PER', 'B-ORG']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a7b393",
   "metadata": {},
   "source": [
    "## Smoothing strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a1cbe5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Add the remaining smoothing strategies\n",
    "def unknown_word_emission(smoothing_strategy: Enum) -> float:\n",
    "    if smoothing_strategy == SmoothingStrategy.first:\n",
    "        return 1 / len(tags)\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585bf04d",
   "metadata": {},
   "source": [
    "## Matrix Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cb7704b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transition_matrix(tags: List[str], train: Iterator[TokenList]) -> np.array:\n",
    "    transition_matrix = np.zeros((len(tags), len(tags)), dtype=float)\n",
    "\n",
    "    tag_counter = defaultdict(int)\n",
    "    transition_counter = defaultdict(int)\n",
    "\n",
    "    for sentence in parse_incr(train):\n",
    "        # count first tag of sentence and match it with 'START' artificial tag\n",
    "        first_tag = sentence[0]['lemma']\n",
    "        tag_counter['START'] += 1\n",
    "        transition_counter[('START', first_tag)] += 1\n",
    "\n",
    "        # count middle token pairs\n",
    "        for (token_a, token_b) in zip(sentence, sentence[1:]):\n",
    "            tag_counter[token_a['lemma']] += 1\n",
    "            transition_counter[(token_a['lemma'], token_b['lemma'])] += 1\n",
    "\n",
    "        # count last tag of sentence\n",
    "        tag_counter[sentence[-1]['lemma']] += 1\n",
    "\n",
    "    for i, t1 in enumerate(tags):\n",
    "        for j, t2 in enumerate(tags):\n",
    "            if tag_counter[t1]:  # if tag occurs at least once\n",
    "                transition_matrix[i][j] = transition_counter[(t1, t2)] / tag_counter[t1]  # compute transition probability\n",
    "\n",
    "    train.seek(0)\n",
    "    return transition_matrix\n",
    "\n",
    "\n",
    "def get_emission_probabilities(train: Iterator[TokenList]) -> Dict[str, str]:\n",
    "    word_tag_counter = defaultdict(int)\n",
    "    tag_counter = defaultdict(int)\n",
    "\n",
    "    for sentence in parse_incr(train):\n",
    "        for token in sentence:\n",
    "            word_tag_counter[(token['form'], token['lemma'])] += 1\n",
    "            tag_counter[token['lemma']] += 1\n",
    "\n",
    "    emission_probabilities = {(word, tag): count / tag_counter[tag] for (word, tag), count in word_tag_counter.items()}  # compute emission probability\n",
    "    train.seek(0)\n",
    "    return emission_probabilities\n",
    "\n",
    "\n",
    "def get_emission_matrix(tags: List[str], words: List[str], emission_probabilities: [Dict[str, str]]) -> np.array:\n",
    "    emission_matrix = np.zeros((len(tags), len(words)), dtype='float32')\n",
    "    for i, tag in enumerate(tags):\n",
    "        for j, word in enumerate(words):\n",
    "            emission_matrix[i, j] = emission_probabilities.get((word, tag), unknown_word_emission(smoothing_strategy))\n",
    "\n",
    "    return emission_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d7705c",
   "metadata": {},
   "source": [
    "## Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "79396733",
   "metadata": {},
   "outputs": [],
   "source": [
    "emission_probabilities = get_emission_probabilities(train)\n",
    "transition_matrix = get_transition_matrix(tags, train)\n",
    "Π = transition_matrix[0, 1:]\n",
    "\n",
    "tags.remove('START')\n",
    "transition_matrix = transition_matrix[1:, 1:]\n",
    "\n",
    "train.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "133f8b21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>O</th>\n",
       "      <th>I-LOC</th>\n",
       "      <th>I-MISC</th>\n",
       "      <th>I-ORG</th>\n",
       "      <th>I-PER</th>\n",
       "      <th>B-LOC</th>\n",
       "      <th>B-MISC</th>\n",
       "      <th>B-PER</th>\n",
       "      <th>B-ORG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>O</th>\n",
       "      <td>0.874402</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001979</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.027177</td>\n",
       "      <td>0.017658</td>\n",
       "      <td>0.018936</td>\n",
       "      <td>0.011958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I-LOC</th>\n",
       "      <td>0.754676</td>\n",
       "      <td>0.244685</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000213</td>\n",
       "      <td>0.000053</td>\n",
       "      <td>0.000320</td>\n",
       "      <td>0.000053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I-MISC</th>\n",
       "      <td>0.462849</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.536522</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000509</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I-ORG</th>\n",
       "      <td>0.586095</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.413267</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000245</td>\n",
       "      <td>0.000392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I-PER</th>\n",
       "      <td>0.876705</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.123159</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000102</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B-LOC</th>\n",
       "      <td>0.739153</td>\n",
       "      <td>0.258169</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>0.002441</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000182</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B-MISC</th>\n",
       "      <td>0.654785</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.321987</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.022703</td>\n",
       "      <td>0.000498</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B-PER</th>\n",
       "      <td>0.358111</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.641541</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000348</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B-ORG</th>\n",
       "      <td>0.503447</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.493953</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000454</td>\n",
       "      <td>0.002147</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               O     I-LOC    I-MISC     I-ORG     I-PER     B-LOC    B-MISC  \\\n",
       "O       0.874402  0.000000  0.001979  0.000000  0.000000  0.027177  0.017658   \n",
       "I-LOC   0.754676  0.244685  0.000000  0.000000  0.000000  0.000213  0.000053   \n",
       "I-MISC  0.462849  0.000030  0.536522  0.000030  0.000060  0.000000  0.000509   \n",
       "I-ORG   0.586095  0.000000  0.000000  0.413267  0.000000  0.000000  0.000000   \n",
       "I-PER   0.876705  0.000000  0.000034  0.000000  0.123159  0.000000  0.000000   \n",
       "B-LOC   0.739153  0.258169  0.000000  0.000000  0.000055  0.002441  0.000000   \n",
       "B-MISC  0.654785  0.000000  0.321987  0.000000  0.000028  0.000000  0.022703   \n",
       "B-PER   0.358111  0.000000  0.000000  0.000000  0.641541  0.000000  0.000000   \n",
       "B-ORG   0.503447  0.000000  0.000000  0.493953  0.000000  0.000000  0.000000   \n",
       "\n",
       "           B-PER     B-ORG  \n",
       "O       0.018936  0.011958  \n",
       "I-LOC   0.000320  0.000053  \n",
       "I-MISC  0.000000  0.000000  \n",
       "I-ORG   0.000245  0.000392  \n",
       "I-PER   0.000102  0.000000  \n",
       "B-LOC   0.000182  0.000000  \n",
       "B-MISC  0.000498  0.000000  \n",
       "B-PER   0.000348  0.000000  \n",
       "B-ORG   0.000454  0.002147  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(transition_matrix, index=tags, columns=tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91abef94",
   "metadata": {},
   "source": [
    "## Definition of Viterbi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "400f10a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Need to use logarithm to compute argmax probability in order to improve performance\n",
    "def argmax(viterbi_matrix: np.array, Tm: np.array, Em: np.array, j: int, i: int) -> Tuple[int, float]:\n",
    "    max_prob = 0\n",
    "    max_index = 0\n",
    "    for index, prob in enumerate(viterbi_matrix[:, i-1]):\n",
    "        prob *= Tm[index, j] * Em[j, i]\n",
    "        if prob > max_prob:\n",
    "            max_index, max_prob = index, prob\n",
    "    \n",
    "    return max_index, max_prob\n",
    "\n",
    "\n",
    "def viterbi(words: List[str], tags: List[str], Π: np.array, Tm: np.array, Em: np.array) -> List[str]:\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    words: List[str]\n",
    "        Words sequence of the sentence\n",
    "    tags: List[str]\n",
    "        List of NER tags\n",
    "    Π: np.array\n",
    "        Array of initial probabilities (1, T)\n",
    "    Tm: np.array\n",
    "        Transition matrix (T, T)\n",
    "    Em: np.array\n",
    "        Emission matrix (T, W)\n",
    "    \"\"\"\n",
    "\n",
    "    W = len(words)\n",
    "    T = len(tags)\n",
    "\n",
    "    viterbi_matrix = np.zeros((T, W), dtype=float)\n",
    "    backpointer = np.empty((T, W), dtype=int)\n",
    "\n",
    "    # compute first word initial probability for each tag\n",
    "    viterbi_matrix[:, 0] = [emission * initial_p for emission, initial_p in zip(Em[:, 0], Π)]\n",
    "\n",
    "    # compute probabilities and fill backpointer for the rest of matrix\n",
    "    for i in range(1, W):\n",
    "        for j in range(T):\n",
    "            k, value = argmax(viterbi_matrix, Tm, Em, j, i)\n",
    "            viterbi_matrix[j, i] = value\n",
    "            backpointer[j, i] = k\n",
    "\n",
    "    # get tag index k of last column with highest probability\n",
    "    last_column = list(viterbi_matrix[:, -1])\n",
    "    max_value = max(last_column)\n",
    "    k = last_column.index(max_value)\n",
    "\n",
    "    # get best path walking through backpointer\n",
    "    best_path = list()\n",
    "    for i in range(W-1, -1, -1):\n",
    "        best_path.append(tags[k])\n",
    "        k = backpointer[k, i]\n",
    "    \n",
    "    best_path.reverse()\n",
    "    return best_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38094270",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3c0e2733",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words: 267155\n",
      "Correctly tagged: 250487\n",
      "Accuracy: 93.76%\n"
     ]
    }
   ],
   "source": [
    "total_words = 0\n",
    "correctly_classified = 0\n",
    "for sentence in parse_incr(test):\n",
    "    total_words += len(sentence)\n",
    "    correct_tags = [token['lemma'] for token in sentence]\n",
    "    words = np.array([token['form'] for token in sentence])\n",
    "\n",
    "    emission_matrix = get_emission_matrix(tags, words, emission_probabilities)\n",
    "    result_tags = viterbi(words, tags, Π, transition_matrix, emission_matrix)\n",
    "    \n",
    "    correctly_classified += sum(x == y for x, y in zip(correct_tags, result_tags))\n",
    "\n",
    "print(f'Total words: {total_words}')\n",
    "print(f'Correctly tagged: {correctly_classified}')\n",
    "print(f'Accuracy: {round(correctly_classified / total_words * 100, 2)}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
